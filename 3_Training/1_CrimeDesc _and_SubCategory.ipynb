{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (4.46.2)\n",
      "Requirement already satisfied: torch in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (2.0.1)\n",
      "Requirement already satisfied: torchtext in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (0.15.2)\n",
      "Requirement already satisfied: scikit-learn in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: pandas in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: filelock in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from transformers) (4.66.6)\n",
      "Requirement already satisfied: typing-extensions in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: torchdata==0.6.1 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from torchtext) (0.6.1)\n",
      "Requirement already satisfied: urllib3>=1.25 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from torchdata==0.6.1->torchtext) (2.2.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\n",
      "Requirement already satisfied: six>=1.5 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: colorama in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==2.0.1 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (2.0.1)\n",
      "Requirement already satisfied: torchtext==0.15.2 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (0.15.2)\n",
      "Requirement already satisfied: filelock in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from torch==2.0.1) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from torch==2.0.1) (4.12.2)\n",
      "Requirement already satisfied: sympy in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from torch==2.0.1) (1.13.3)\n",
      "Requirement already satisfied: networkx in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from torch==2.0.1) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from torch==2.0.1) (3.1.4)\n",
      "Requirement already satisfied: tqdm in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from torchtext==0.15.2) (4.66.6)\n",
      "Requirement already satisfied: requests in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from torchtext==0.15.2) (2.32.3)\n",
      "Requirement already satisfied: numpy in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from torchtext==0.15.2) (1.26.4)\n",
      "Requirement already satisfied: torchdata==0.6.1 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from torchtext==0.15.2) (0.6.1)\n",
      "Requirement already satisfied: urllib3>=1.25 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from torchdata==0.6.1->torchtext==0.15.2) (2.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from jinja2->torch==2.0.1) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from requests->torchtext==0.15.2) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from requests->torchtext==0.15.2) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from requests->torchtext==0.15.2) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from sympy->torch==2.0.1) (1.3.0)\n",
      "Requirement already satisfied: colorama in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from tqdm->torchtext==0.15.2) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (2.18.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.18.0 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (5.28.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.67.1)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.6.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: rich in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (13.9.4)\n",
      "Requirement already satisfied: namex in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.13.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (4.46.2)Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: torch in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (2.0.1)\n",
      "Requirement already satisfied: scikit-learn in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: filelock in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from transformers) (4.66.6)\n",
      "Requirement already satisfied: typing-extensions in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\n",
      "Requirement already satisfied: colorama in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: datasets in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (3.1.0)\n",
      "Requirement already satisfied: filelock in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from datasets) (18.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from datasets) (4.66.6)\n",
      "Requirement already satisfied: xxhash in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec[http]<=2024.9.0,>=2023.1.0 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from datasets) (3.10.10)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from datasets) (0.26.2)\n",
      "Requirement already satisfied: packaging in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from aiohttp->datasets) (1.17.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: colorama in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in g:\\cyberguard ai\\cyber_venv\\lib\\site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers torch torchtext scikit-learn pandas\n",
    "%pip install torch==2.0.1 torchtext==0.15.2\n",
    "!pip install tensorflow\n",
    "%pip install transformers torch scikit-learn\n",
    "!pip install datasets\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\CyberGuard AI\\cyber_venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              sub_category  \\\n",
      "0                       intimidating email   \n",
      "1  business email compromiseemail takeover   \n",
      "2                       hacking/defacement   \n",
      "3           unauthorised accessdata breach   \n",
      "4                           email phishing   \n",
      "\n",
      "                                 crimeadditionalinfo  \n",
      "0   some unknown user has send an email to my rwa...  \n",
      "1   millennium semiconductors is into business of...  \n",
      "2   my bank account was transction, and then i go...  \n",
      "3   on feb    i installed koko loan reliable rupe...  \n",
      "4   i met a friend in online game his name is kai...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "g:\\CyberGuard AI\\cyber_venv\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 374/374 [33:29<00:00,  5.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 2.8661\n",
      "Epoch 2/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 374/374 [49:50<00:00,  7.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 2.0023\n",
      "Epoch 3/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 374/374 [38:37<00:00,  6.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 1.4504\n",
      "Epoch 4/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 374/374 [34:42<00:00,  5.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 1.0716\n",
      "Epoch 5/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 374/374 [20:54<00:00,  3.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.8228\n",
      "Epoch 6/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 374/374 [20:57<00:00,  3.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.6542\n",
      "Epoch 7/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 374/374 [20:59<00:00,  3.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.5730\n",
      "Epoch 8/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 374/374 [20:56<00:00,  3.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.5267\n",
      "Epoch 9/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 374/374 [20:54<00:00,  3.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.4988\n",
      "Epoch 10/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 374/374 [20:55<00:00,  3.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.4902\n",
      "Epoch 11/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 374/374 [21:04<00:00,  3.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.4492\n",
      "Epoch 12/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 374/374 [23:49<00:00,  3.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.4339\n",
      "Epoch 13/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 374/374 [20:57<00:00,  3.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.4402\n",
      "Epoch 14/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 374/374 [20:56<00:00,  3.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.4340\n",
      "Epoch 15/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 374/374 [22:50<00:00,  3.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.3872\n",
      "Epoch 16/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 374/374 [20:58<00:00,  3.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.3803\n",
      "Epoch 17/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 374/374 [21:08<00:00,  3.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.3754\n",
      "Epoch 18/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 374/374 [21:02<00:00,  3.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.3633\n",
      "Epoch 19/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 374/374 [21:07<00:00,  3.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.3606\n",
      "Epoch 20/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 374/374 [21:08<00:00,  3.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.3361\n",
      "Epoch 21/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 374/374 [21:08<00:00,  3.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.3078\n",
      "Epoch 22/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 374/374 [21:10<00:00,  3.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.3128\n",
      "Epoch 23/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 374/374 [21:11<00:00,  3.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.3118\n",
      "Epoch 24/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 374/374 [21:13<00:00,  3.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.3154\n",
      "Epoch 25/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 374/374 [21:12<00:00,  3.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.2904\n",
      "Epoch 26/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 374/374 [21:10<00:00,  3.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.2782\n",
      "Epoch 27/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 374/374 [21:00<00:00,  3.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.2735\n",
      "Epoch 28/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 374/374 [20:55<00:00,  3.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.3017\n",
      "Epoch 29/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 374/374 [20:57<00:00,  3.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.3006\n",
      "Epoch 30/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 374/374 [20:59<00:00,  3.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.2584\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('trained_bert_tokenizer\\\\tokenizer_config.json',\n",
       " 'trained_bert_tokenizer\\\\special_tokens_map.json',\n",
       " 'trained_bert_tokenizer\\\\vocab.txt',\n",
       " 'trained_bert_tokenizer\\\\added_tokens.json')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import torch \n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load your data \n",
    "# df = pd.read_csv(\"final_processed.csv\") \n",
    "df = pd.read_csv(\"sampled_data.csv\") \n",
    "\n",
    "# Drop rows with NaN values\n",
    "df = df.dropna()\n",
    "\n",
    "# Display the DataFrame to verify the data \n",
    "print(df.head()) \n",
    "\n",
    "# Map labels to integers \n",
    "label_to_int = {label: i for i, label in enumerate(df['sub_category'].unique())} \n",
    "df['sub_category'] = df['sub_category'].map(label_to_int) \n",
    "\n",
    "# Set hyperparameters \n",
    "MAX_LENGTH = 128 \n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 30\n",
    "LEARNING_RATE = 5e-5\n",
    "\n",
    "# Initialize tokenizer \n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Define CustomTextDataset \n",
    "class CustomTextDataset(Dataset): \n",
    "    def __init__(self, dataframe, tokenizer, max_length, text_column, label_column): \n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.text_column = text_column\n",
    "        self.label_column = label_column\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        # Get text and label for the sample \n",
    "        text = self.data.iloc[idx][self.text_column]\n",
    "        label = self.data.iloc[idx][self.label_column]\n",
    "\n",
    "        # Tokenize the text\n",
    "        tokens = self.tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ) \n",
    "\n",
    "        # Remove the batch dimension from token tensors \n",
    "        tokens = {key: val.squeeze(0) for key, val in tokens.items()} \n",
    "\n",
    "        # Return tokenized inputs and the label \n",
    "        return tokens, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "def predict(model, tokenizer, texts, max_length=128):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for text in texts:\n",
    "            tokens = tokenizer( \n",
    "                text, \n",
    "                padding='max_length', \n",
    "                max_length=max_length,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            tokens = {key: val.to(model.device) for key, val in tokens.items()}\n",
    "            output = model(**tokens)\n",
    "            _, predicted_class = torch.max(output.logits, dim=1)\n",
    "            predictions.append(predicted_class.item())\n",
    "    return predictions\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "unique_labels = df['sub_category'].nunique()\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=unique_labels)\n",
    "\n",
    "train_dataset = CustomTextDataset(df, tokenizer, MAX_LENGTH, text_column='crimeadditionalinfo', label_column='sub_category')\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}\") \n",
    "    total_loss = 0 \n",
    "    for batch in tqdm(train_loader): \n",
    "        optimizer.zero_grad()\n",
    "        tokens, labels = batch\n",
    "        labels = labels.to(torch.long)  \n",
    "        outputs = model(**tokens, labels=labels)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        loss.backward() \n",
    "        optimizer.step() \n",
    "        total_loss += loss.item() \n",
    "\n",
    "    average_loss = total_loss / len(train_loader) \n",
    "    print(f\"Average Loss: {average_loss:.4f}\") \n",
    "\n",
    "# Save the trained model\n",
    "model.save_pretrained(\"trained_bert_model\")\n",
    "tokenizer.save_pretrained(\"trained_bert_tokenizer\")\n",
    "\n",
    "# Predict on unknown texts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m model \u001b[38;5;241m=\u001b[39m BertForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrained_bert_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m BertTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrained_bert_tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m label_to_int \u001b[38;5;241m=\u001b[39m {label: i \u001b[38;5;28;01mfor\u001b[39;00m i, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mdf\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msub_category\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique())} \n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(model, tokenizer, texts, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m):\n\u001b[0;32m     17\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# unknown_texts = [\n",
    " \n",
    "#     \"received URL link for updating KYC from mobile no  after opening received otp and entered and confirmation received saying  KYC done like this received OTP till  KYC after that started receiving sms from bank after  sec delay on amount cut and then stopped further otp Bank is also responsible for delaying sms\"\n",
    "# ]\n",
    "# predicted_classes = predict(model, tokenizer, unknown_texts)\n",
    "\n",
    "# int_to_label = {v: k for k, v in label_to_int.items()}\n",
    "\n",
    "# for text, prediction in zip(unknown_texts, predicted_classes): \n",
    "#     predicted_label = int_to_label[prediction]\n",
    "#     print(f\"Text: {text} | Predicted Class: {prediction} | Class Label: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              sub_category  \\\n",
      "0                       intimidating email   \n",
      "1  business email compromiseemail takeover   \n",
      "2                       hacking/defacement   \n",
      "3           unauthorised accessdata breach   \n",
      "4                           email phishing   \n",
      "\n",
      "                                 crimeadditionalinfo  \n",
      "0   some unknown user has send an email to my rwa...  \n",
      "1   millennium semiconductors is into business of...  \n",
      "2   my bank account was transction, and then i go...  \n",
      "3   on feb    i installed koko loan reliable rupe...  \n",
      "4   i met a friend in online game his name is kai...  \n",
      "Text: received URL link for updating KYC from mobile no after opening received otp and entered and confirmation received saying KYC done like this received OTP till KYC after that started receiving sms from bank after sec delay on amount cut and then stopped further otp Bank is also responsible for delaying sms\n",
      "Predicted Class: 17 | Class Label: internet banking related fraud\n",
      "\n",
      "Text: some unknown user has send an email to my rwa and making my reputation at stake along with my family . i request you to please take a legal action and help me to find the person who has is ruing my personal reputation .\n",
      "Predicted Class: 0 | Class Label: intimidating email\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import pandas as pd \n",
    "import torch \n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load your data \n",
    "# df = pd.read_csv(\"final_processed.csv\") \n",
    "df = pd.read_csv(\"sampled_data.csv\") \n",
    "\n",
    "# Drop rows with NaN values\n",
    "df = df.dropna()\n",
    "\n",
    "# Display the DataFrame to verify the data \n",
    "print(df.head()) \n",
    "\n",
    "# Map labels to integers \n",
    "label_to_int = {label: i for i, label in enumerate(df['sub_category'].unique())} \n",
    "df['sub_category'] = df['sub_category'].map(label_to_int) \n",
    "\n",
    "# Load the trained model and tokenizer\n",
    "model = BertForSequenceClassification.from_pretrained(\"trained_bert_model\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"trained_bert_tokenizer\")\n",
    "\n",
    "# Ensure model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "\n",
    "int_to_label = {v: k for k, v in label_to_int.items()}\n",
    "\n",
    "# Define the prediction function\n",
    "def predict(model, tokenizer, texts, max_length=128):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for text in texts:\n",
    "            # Tokenize the input text\n",
    "            tokens = tokenizer(\n",
    "                text,\n",
    "                padding='max_length',\n",
    "                max_length=max_length,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            tokens = {key: val.to(model.device) for key, val in tokens.items()}\n",
    "            \n",
    "            # Get model outputs\n",
    "            output = model(**tokens)\n",
    "            _, predicted_class = torch.max(output.logits, dim=1)\n",
    "            predictions.append(predicted_class.item())\n",
    "    return predictions\n",
    "\n",
    "# Input texts to classify\n",
    "unknown_texts = [\n",
    "    \"received URL link for updating KYC from mobile no after opening received otp and entered and confirmation received saying KYC done like this received OTP till KYC after that started receiving sms from bank after sec delay on amount cut and then stopped further otp Bank is also responsible for delaying sms\",\n",
    "    \"some unknown user has send an email to my rwa and making my reputation at stake along with my family . i request you to please take a legal action and help me to find the person who has is ruing my personal reputation .\"\n",
    "]\n",
    "\n",
    "# Perform prediction\n",
    "predicted_classes = predict(model, tokenizer, unknown_texts)\n",
    "\n",
    "# Map predictions back to label names\n",
    "for text, prediction in zip(unknown_texts, predicted_classes):\n",
    "    predicted_label = int_to_label[prediction]\n",
    "    print(f\"Text: {text}\\nPredicted Class: {prediction} | Class Label: {predicted_label}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'label_to_int' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 43\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m predictions\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Mapping integers back to their original labels\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m int_to_label \u001b[38;5;241m=\u001b[39m {v: k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[43mlabel_to_int\u001b[49m\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Example unknown texts for prediction\u001b[39;00m\n\u001b[0;32m     46\u001b[0m unknown_texts \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMy bank account was hacked, and I lost access to everything. Please help me recover it.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived a phishing email asking me to update my account details.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     49\u001b[0m ]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'label_to_int' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# Load the trained model and tokenizer\n",
    "model = BertForSequenceClassification.from_pretrained(\"trained_bert_model\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"trained_bert_tokenizer\")\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Define the predict function\n",
    "def predict(model, tokenizer, texts, max_length=128):\n",
    "    \"\"\"\n",
    "    Predict the labels for the given texts using the trained model.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained BERT model.\n",
    "        tokenizer: Tokenizer used with the BERT model.\n",
    "        texts: List of input texts to classify.\n",
    "        max_length: Maximum sequence length for padding and truncation.\n",
    "    \n",
    "    Returns:\n",
    "        List of predicted labels.\n",
    "    \"\"\"\n",
    "    model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for text in texts:\n",
    "            tokens = tokenizer(\n",
    "                text,\n",
    "                padding='max_length',\n",
    "                max_length=max_length,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            tokens = {key: val.to(model.device) for key, val in tokens.items()}\n",
    "            output = model(**tokens)\n",
    "            _, predicted_class = torch.max(output.logits, dim=1)\n",
    "            predictions.append(predicted_class.item())\n",
    "    return predictions\n",
    "\n",
    "# Mapping integers back to their original labels\n",
    "int_to_label = {v: k for k, v in label_to_int.items()}\n",
    "\n",
    "# Example unknown texts for prediction\n",
    "unknown_texts = [\n",
    "    \"My bank account was hacked, and I lost access to everything. Please help me recover it.\",\n",
    "    \"Received a phishing email asking me to update my account details.\",\n",
    "]\n",
    "\n",
    "# Get predictions\n",
    "predicted_classes = predict(model, tokenizer, unknown_texts)\n",
    "\n",
    "# Display predictions with their corresponding labels\n",
    "for text, prediction in zip(unknown_texts, predicted_classes):\n",
    "    predicted_label = int_to_label[prediction]\n",
    "    print(f\"Text: {text} | Predicted Class: {prediction} | Class Label: {predicted_label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                category                       sub_category  \\\n",
      "0  Online and Social Media Related Crime  cyber bullying  stalking  sexting   \n",
      "1                 Online Financial Fraud                  fraud callvishing   \n",
      "2               Online Gambling  Betting           online gambling  betting   \n",
      "3  Online and Social Media Related Crime                   online job fraud   \n",
      "4                 Online Financial Fraud                  fraud callvishing   \n",
      "\n",
      "                                  crimeaditionalinfo  \n",
      "0  I had continue received random calls and abusi...  \n",
      "1  The above fraudster is continuously messaging ...  \n",
      "2  He is acting like a police and demanding for m...  \n",
      "3  In apna Job I have applied for job interview f...  \n",
      "4  I received a call from lady stating that she w...  \n",
      "Accuracy: 0.9992125984251968\n",
      "\n",
      "Classification Report:\n",
      "                                                 precision    recall  f1-score   support\n",
      "\n",
      "                          Cryptocurrency Crime       1.00      1.00      1.00        88\n",
      "                Cyber Attack/ Dependent Crimes       0.98      1.00      0.99       730\n",
      "                               Cyber Terrorism       1.00      1.00      1.00        32\n",
      "Hacking  Damage to computercomputer system etc       1.00      0.97      0.98       349\n",
      "                      Online Cyber Trafficking       1.00      1.00      1.00        27\n",
      "                        Online Financial Fraud       1.00      1.00      1.00     11497\n",
      "                      Online Gambling  Betting       1.00      1.00      1.00        83\n",
      "         Online and Social Media Related Crime       1.00      1.00      1.00      2420\n",
      "                                    Ransomware       1.00      1.00      1.00        14\n",
      "\n",
      "                                      accuracy                           1.00     15240\n",
      "                                     macro avg       1.00      1.00      1.00     15240\n",
      "                                  weighted avg       1.00      1.00      1.00     15240\n",
      "\n",
      "\n",
      "Predicted Category: Online and Social Media Related Crime\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cyber_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

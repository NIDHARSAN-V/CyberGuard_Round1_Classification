{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers torch torchtext scikit-learn pandas\n",
    "%pip install torch==2.0.1 torchtext==0.15.2\n",
    "!pip install tensorflow\n",
    "%pip install transformers torch scikit-learn\n",
    "!pip install datasets\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import torch \n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load your data \n",
    "df = pd.read_csv(\"crime_data.csv\") \n",
    "\n",
    "# Drop rows with NaN values\n",
    "df = df.dropna()\n",
    "\n",
    "# Display the DataFrame to verify the data \n",
    "print(df.head()) \n",
    "\n",
    "# Map labels to integers \n",
    "label_to_int = {label: i for i, label in enumerate(df['sub_category'].unique())} \n",
    "df['sub_category'] = df['sub_category'].map(label_to_int) \n",
    "\n",
    "# Set hyperparameters \n",
    "MAX_LENGTH = 128 \n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 30\n",
    "LEARNING_RATE = 5e-5\n",
    "\n",
    "# Initialize tokenizer \n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Define CustomTextDataset \n",
    "class CustomTextDataset(Dataset): \n",
    "    def __init__(self, dataframe, tokenizer, max_length, text_column, label_column): \n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.text_column = text_column\n",
    "        self.label_column = label_column\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        # Get text and label for the sample \n",
    "        text = self.data.iloc[idx][self.text_column]\n",
    "        label = self.data.iloc[idx][self.label_column]\n",
    "\n",
    "        # Tokenize the text\n",
    "        tokens = self.tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ) \n",
    "\n",
    "        # Remove the batch dimension from token tensors \n",
    "        tokens = {key: val.squeeze(0) for key, val in tokens.items()} \n",
    "\n",
    "        # Return tokenized inputs and the label \n",
    "        return tokens, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "def predict(model, tokenizer, texts, max_length=128):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for text in texts:\n",
    "            tokens = tokenizer( \n",
    "                text, \n",
    "                padding='max_length', \n",
    "                max_length=max_length,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            tokens = {key: val.to(model.device) for key, val in tokens.items()}\n",
    "            output = model(**tokens)\n",
    "            _, predicted_class = torch.max(output.logits, dim=1)\n",
    "            predictions.append(predicted_class.item())\n",
    "    return predictions\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "unique_labels = df['sub_category'].nunique()\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=unique_labels)\n",
    "\n",
    "train_dataset = CustomTextDataset(df, tokenizer, MAX_LENGTH, text_column='crimeaditionalinfo', label_column='sub_category')\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}\") \n",
    "    total_loss = 0 \n",
    "    for batch in tqdm(train_loader): \n",
    "        optimizer.zero_grad()\n",
    "        tokens, labels = batch\n",
    "        labels = labels.to(torch.long)  \n",
    "        outputs = model(**tokens, labels=labels)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        loss.backward() \n",
    "        optimizer.step() \n",
    "        total_loss += loss.item() \n",
    "\n",
    "    average_loss = total_loss / len(train_loader) \n",
    "    print(f\"Average Loss: {average_loss:.4f}\") \n",
    "\n",
    "# Save the trained model\n",
    "model.save_pretrained(\"trained_bert_model\")\n",
    "tokenizer.save_pretrained(\"trained_bert_tokenizer\")\n",
    "\n",
    "# Predict on unknown texts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_texts = [\n",
    " \n",
    "    \"spam message i recieve msg from unwanted number they say you take loan and today is repayment date but i did not take loani recieve text and whatsapp message any time for make repayment of loan but i did not take any loan \"\n",
    "]\n",
    "predicted_classes = predict(model, tokenizer, unknown_texts)\n",
    "\n",
    "int_to_label = {v: k for k, v in label_to_int.items()}\n",
    "\n",
    "for text, prediction in zip(unknown_texts, predicted_classes): \n",
    "    predicted_label = int_to_label[prediction]\n",
    "    print(f\"Text: {text} | Predicted Class: {prediction} | Class Label: {predicted_label}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
